import os
import sys
import time
import threading
import numpy as np
import torch


sys.path.append("..")  # æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´
from utils.dataset import getDataset
from models.tracker_eval import TrackerNetEval

sys.path.append("/usr/lib/python3/dist-packages/")

from metavision_core.event_io import EventsIterator
from metavision_core.event_io.raw_reader import initiate_device
from metavision_sdk_core import PeriodicFrameGenerationAlgorithm, ColorPalette
from metavision_sdk_ui import EventLoop, BaseWindow, MTWindow, UIKeyEvent



CAMERA_CONFIGS = {
    "left": {"serial": "00051195", "mode": "slave"},
    "right": {"serial": "00051197", "mode": "master"}
}


def get_event_frame_generator(serial):
    device = initiate_device(path=serial)
    mv_iterator = EventsIterator.from_device(device=device)
    height, width = mv_iterator.get_size()
    frame_gen = PeriodicFrameGenerationAlgorithm(width, height, fps=300, palette=ColorPalette.CoolWarm)

    frames = []

    def on_frame_cb(ts, frame):
        frames.append(frame.copy())

    frame_gen.set_output_callback(on_frame_cb)

    def generator():
        for evs in mv_iterator:
            EventLoop.poll_and_dispatch()
            frame_gen.process_events(evs)
            if frames:
                yield frames.pop(0)

    return generator()


def main():
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'  # ä¿®æ”¹ä¸ºä½ ç”¨çš„ GPU

    print("ğŸš€ Loading model...")
    model = TrackerNetEval(feature_dim=384, hgnn=True)
    model.load_state_dict(torch.load('./ckpt.pth')["state_dict"])
    model = model.cuda().eval()
    print("âœ… Model loaded.")

    print("ğŸ“¦ Loading dataset for geometry info...")
    dataset = getDataset(data_folder="./data", train=False)
    td = dataset[0]  # åªç”¨ç¬¬ä¸€ä¸ª sequence çš„ geometry å·¥å…·
    ref_patch = td[0][1]['ref_img'].cuda()
    current_pos_l = td[0][1]['u_centers_l'].cuda()

    pred = None
    pos_3d = []
    pos_l = []
    disp = []

    print("ğŸ“¡ Initializing cameras...")
    left_stream = get_event_frame_generator(CAMERA_CONFIGS["left"]["serial"])
    right_stream = get_event_frame_generator(CAMERA_CONFIGS["right"]["serial"])

    print("ğŸ¬ Starting inference loop...")
    try:
        for idx, (ev_frame_l_np, ev_frame_r_np) in enumerate(zip(left_stream, right_stream)):
            if idx >= 100:  # åªå¤„ç†å‰100å¸§ï¼Œå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹
                break

            # è½¬æ¢æˆæ¨¡å‹è¾“å…¥æ ¼å¼
            ev_frame_l = torch.from_numpy(ev_frame_l_np).float().unsqueeze(0).unsqueeze(0).cuda() / 255.
            ev_frame_r = torch.from_numpy(ev_frame_r_np).float().unsqueeze(0).unsqueeze(0).cuda() / 255.

            with torch.no_grad():
                flow_l_pred, disp_pred, pred = model(
                    ev_frame_l, ev_frame_r, ref_patch, current_pos_l, None, pred=pred
                )
                current_pos_l += flow_l_pred.detach()

                pos = td.reprojectImageTo3D_ph(disp_pred[0].cpu(), current_pos_l[0].cpu())

                pos_3d.append(pos)
                pos_l.append(current_pos_l.clone())
                disp.append(disp_pred)

                print(f"ğŸ§  Frame {idx}: 3D points estimated.")

    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user.")

    print("ğŸ’¾ Saving results...")
    os.makedirs('./output/realtime', exist_ok=True)
    np.save('./output/realtime/pos_3d_pred.npy', np.array(pos_3d))
    print("âœ… Saved: pos_3d_pred.npy")


if __name__ == "__main__":
    main()
